
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # Objective: running a joint model combining IF and RE to estimate all interactions at the same time. 
> # 
> # Run in Terminal: 
> #   
> #   R CMD BATCH '--args 0' model.R model/0_model_script.Rout
> # 
> # substituting '0' for the comm number (0, 1, 2, or 3) 
> # 
> # This calls up model.R, which runs joint_model.stan. 
> # results got to model/output, /transformed and /validation
> 
> 
> # Top level script for analysis 4: Estimating interactions with a joint model
> 
> # One model to rule them all: 
> # 1. Estimate observed interactions with IFM 
> # 2. Estimate response and effect from observed interactions
> 
> # Then: 
> # 3. Extract the intrinsic growth rates
> # 4. Estimate missing interactions using response and effect 
> # 5. Scale the interactions into alphas
> 
> # PRELUDE
> #-------
> Sys.time()
[1] "2020-04-06 12:52:36 AEST"
> 
> # Get arguments from bash script
> #!/usr/bin/env Rscript
> args = commandArgs(trailingOnly=TRUE)
> # take comm name as an argument from bash
> if (length(args)==0) {
+   stop("At least one argument must be supplied (input file).n", call.=FALSE)
+ } 
> if (length(args)>1) {
+   stop("Model can only be run on 1 comm at a time.n", call.=FALSE)
+ }
> comm <- args[1]
> 
> # set up R environment
> library(rstan)
Loading required package: StanHeaders
Loading required package: ggplot2
rstan (Version 2.19.3, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores()) 
> 
> library(rethinking)
Loading required package: parallel
rethinking (Version 1.59)
> library(reshape2)
> 
> 
> setwd('~/Dropbox/Work/Projects/2018_Compnet/stormland/')
> 
> source('functions/model/rem_dataprep.R')
> source('functions/model/stan_modelcheck_rem.R')
> source('functions/model/scale_interactions.R')
> 
> # prepare data
> #-------------
> fecundities <- read.csv(paste0('clean_data/fecundities', comm, '.csv'), stringsAsFactors = F)
> 
> stan.data <- rem_dataprep(fecundities)
> 
> # I need to keep note of how focals and neighbours are indexed
> key_speciesID <- unlist(read.csv(paste0('clean_data/key_speciesID', comm, '.csv'), stringsAsFactors = F))
> key_neighbourID <- unlist(read.csv(paste0('clean_data/key_neighbourID', comm, '.csv'), stringsAsFactors = F))
> 
> message(paste0('Community selected: ', comm))
Community selected: 0
> message(paste0('Fecundity data dimensions = ', dim(fecundities)[1], ', ', dim(fecundities)[2]))
Fecundity data dimensions = 5736, 56
> message(paste0('Number of focals = ', length(key_speciesID)))
Number of focals = 22
> message(paste0('Number of neighbours = ', length(key_neighbourID)))
Number of neighbours = 52
> 
> #---------------------------------------------------
> # Estimate interactions with a joint IFM*REM model |
> #---------------------------------------------------
> 
> fit <- stan(file = 'joint_model.stan',
+             data =  stan.data,               # named list of data
+             chains = 4,
+             warmup = 1000,          # number of warmup iterations per chain
+             iter = 5000,            # total number of iterations per chain
+             refresh = 100,         # show progress every 'refresh' iterations
+             control = list(max_treedepth = 10)
+ )

SAMPLING FOR MODEL 'joint_model' NOW (CHAIN 1).

SAMPLING FOR MODEL 'joint_model' NOW (CHAIN 2).

SAMPLING FOR MODEL 'joint_model' NOW (CHAIN 3).

SAMPLING FOR MODEL 'joint_model' NOW (CHAIN 4).
Chain 1: 
Chain 1: Gradient evaluation took 0.008196 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 81.96 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: 
Chain 2: Gradient evaluation took 0.008234 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 82.34 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 4: 
Chain 4: Gradient evaluation took 0.009072 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 90.72 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 3: 
Chain 3: Gradient evaluation took 0.009729 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 97.29 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
Chain 3: Iteration:    1 / 5000 [  0%]  (Warmup)
Chain 4: Iteration:    1 / 5000 [  0%]  (Warmup)
Chain 2: Iteration:  100 / 5000 [  2%]  (Warmup)
Chain 1: Iteration:  100 / 5000 [  2%]  (Warmup)
Chain 3: Iteration:  100 / 5000 [  2%]  (Warmup)
Chain 4: Iteration:  100 / 5000 [  2%]  (Warmup)
Chain 3: Iteration:  200 / 5000 [  4%]  (Warmup)
Chain 4: Iteration:  200 / 5000 [  4%]  (Warmup)
Chain 1: Iteration:  200 / 5000 [  4%]  (Warmup)
Chain 2: Iteration:  200 / 5000 [  4%]  (Warmup)
Chain 3: Iteration:  300 / 5000 [  6%]  (Warmup)
Chain 4: Iteration:  300 / 5000 [  6%]  (Warmup)
Chain 1: Iteration:  300 / 5000 [  6%]  (Warmup)
Chain 3: Iteration:  400 / 5000 [  8%]  (Warmup)
Chain 4: Iteration:  400 / 5000 [  8%]  (Warmup)
Chain 2: Iteration:  300 / 5000 [  6%]  (Warmup)
Chain 1: Iteration:  400 / 5000 [  8%]  (Warmup)
Chain 3: Iteration:  500 / 5000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 5000 [  8%]  (Warmup)
Chain 4: Iteration:  500 / 5000 [ 10%]  (Warmup)
Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
Chain 3: Iteration:  600 / 5000 [ 12%]  (Warmup)
Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
Chain 4: Iteration:  600 / 5000 [ 12%]  (Warmup)
Chain 1: Iteration:  600 / 5000 [ 12%]  (Warmup)
Chain 3: Iteration:  700 / 5000 [ 14%]  (Warmup)
Chain 4: Iteration:  700 / 5000 [ 14%]  (Warmup)
Chain 2: Iteration:  600 / 5000 [ 12%]  (Warmup)
Chain 1: Iteration:  700 / 5000 [ 14%]  (Warmup)
Chain 3: Iteration:  800 / 5000 [ 16%]  (Warmup)
Chain 4: Iteration:  800 / 5000 [ 16%]  (Warmup)
Chain 2: Iteration:  700 / 5000 [ 14%]  (Warmup)
Chain 1: Iteration:  800 / 5000 [ 16%]  (Warmup)
Chain 3: Iteration:  900 / 5000 [ 18%]  (Warmup)
Chain 4: Iteration:  900 / 5000 [ 18%]  (Warmup)
Chain 2: Iteration:  800 / 5000 [ 16%]  (Warmup)
Chain 1: Iteration:  900 / 5000 [ 18%]  (Warmup)
Chain 3: Iteration: 1000 / 5000 [ 20%]  (Warmup)
Chain 3: Iteration: 1001 / 5000 [ 20%]  (Sampling)
Chain 2: Iteration:  900 / 5000 [ 18%]  (Warmup)
Chain 4: Iteration: 1000 / 5000 [ 20%]  (Warmup)
Chain 4: Iteration: 1001 / 5000 [ 20%]  (Sampling)
Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
Chain 1: Iteration: 1001 / 5000 [ 20%]  (Sampling)
Chain 3: Iteration: 1100 / 5000 [ 22%]  (Sampling)
Chain 4: Iteration: 1100 / 5000 [ 22%]  (Sampling)
Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
Chain 2: Iteration: 1001 / 5000 [ 20%]  (Sampling)
Chain 1: Iteration: 1100 / 5000 [ 22%]  (Sampling)
Chain 3: Iteration: 1200 / 5000 [ 24%]  (Sampling)
Chain 4: Iteration: 1200 / 5000 [ 24%]  (Sampling)
Chain 2: Iteration: 1100 / 5000 [ 22%]  (Sampling)
Chain 1: Iteration: 1200 / 5000 [ 24%]  (Sampling)
Chain 3: Iteration: 1300 / 5000 [ 26%]  (Sampling)
Chain 4: Iteration: 1300 / 5000 [ 26%]  (Sampling)
Chain 2: Iteration: 1200 / 5000 [ 24%]  (Sampling)
Chain 1: Iteration: 1300 / 5000 [ 26%]  (Sampling)
Chain 4: Iteration: 1400 / 5000 [ 28%]  (Sampling)
Chain 3: Iteration: 1400 / 5000 [ 28%]  (Sampling)
Chain 2: Iteration: 1300 / 5000 [ 26%]  (Sampling)
Chain 1: Iteration: 1400 / 5000 [ 28%]  (Sampling)
Chain 4: Iteration: 1500 / 5000 [ 30%]  (Sampling)
Chain 2: Iteration: 1400 / 5000 [ 28%]  (Sampling)
Chain 3: Iteration: 1500 / 5000 [ 30%]  (Sampling)
Chain 1: Iteration: 1500 / 5000 [ 30%]  (Sampling)
Chain 4: Iteration: 1600 / 5000 [ 32%]  (Sampling)
Chain 2: Iteration: 1500 / 5000 [ 30%]  (Sampling)
Chain 3: Iteration: 1600 / 5000 [ 32%]  (Sampling)
Chain 1: Iteration: 1600 / 5000 [ 32%]  (Sampling)
Chain 4: Iteration: 1700 / 5000 [ 34%]  (Sampling)
Chain 2: Iteration: 1600 / 5000 [ 32%]  (Sampling)
Chain 3: Iteration: 1700 / 5000 [ 34%]  (Sampling)
Chain 1: Iteration: 1700 / 5000 [ 34%]  (Sampling)
Chain 4: Iteration: 1800 / 5000 [ 36%]  (Sampling)
Chain 2: Iteration: 1700 / 5000 [ 34%]  (Sampling)
Chain 1: Iteration: 1800 / 5000 [ 36%]  (Sampling)
Chain 3: Iteration: 1800 / 5000 [ 36%]  (Sampling)
Chain 4: Iteration: 1900 / 5000 [ 38%]  (Sampling)
Chain 2: Iteration: 1800 / 5000 [ 36%]  (Sampling)
Chain 1: Iteration: 1900 / 5000 [ 38%]  (Sampling)
Chain 3: Iteration: 1900 / 5000 [ 38%]  (Sampling)
Chain 4: Iteration: 2000 / 5000 [ 40%]  (Sampling)
Chain 2: Iteration: 1900 / 5000 [ 38%]  (Sampling)
Chain 1: Iteration: 2000 / 5000 [ 40%]  (Sampling)
Chain 3: Iteration: 2000 / 5000 [ 40%]  (Sampling)
Chain 4: Iteration: 2100 / 5000 [ 42%]  (Sampling)
Chain 2: Iteration: 2000 / 5000 [ 40%]  (Sampling)
Chain 1: Iteration: 2100 / 5000 [ 42%]  (Sampling)
Chain 3: Iteration: 2100 / 5000 [ 42%]  (Sampling)
Chain 4: Iteration: 2200 / 5000 [ 44%]  (Sampling)
Chain 2: Iteration: 2100 / 5000 [ 42%]  (Sampling)
Chain 1: Iteration: 2200 / 5000 [ 44%]  (Sampling)
Chain 3: Iteration: 2200 / 5000 [ 44%]  (Sampling)
Chain 4: Iteration: 2300 / 5000 [ 46%]  (Sampling)
Chain 2: Iteration: 2200 / 5000 [ 44%]  (Sampling)
Chain 1: Iteration: 2300 / 5000 [ 46%]  (Sampling)
Chain 3: Iteration: 2300 / 5000 [ 46%]  (Sampling)
Chain 4: Iteration: 2400 / 5000 [ 48%]  (Sampling)
Chain 2: Iteration: 2300 / 5000 [ 46%]  (Sampling)
Chain 1: Iteration: 2400 / 5000 [ 48%]  (Sampling)
Chain 3: Iteration: 2400 / 5000 [ 48%]  (Sampling)
Chain 4: Iteration: 2500 / 5000 [ 50%]  (Sampling)
Chain 2: Iteration: 2400 / 5000 [ 48%]  (Sampling)
Chain 1: Iteration: 2500 / 5000 [ 50%]  (Sampling)
Chain 3: Iteration: 2500 / 5000 [ 50%]  (Sampling)
Chain 4: Iteration: 2600 / 5000 [ 52%]  (Sampling)
Chain 2: Iteration: 2500 / 5000 [ 50%]  (Sampling)
Chain 1: Iteration: 2600 / 5000 [ 52%]  (Sampling)
Chain 3: Iteration: 2600 / 5000 [ 52%]  (Sampling)
Chain 4: Iteration: 2700 / 5000 [ 54%]  (Sampling)
Chain 2: Iteration: 2600 / 5000 [ 52%]  (Sampling)
Chain 1: Iteration: 2700 / 5000 [ 54%]  (Sampling)
Chain 4: Iteration: 2800 / 5000 [ 56%]  (Sampling)
Chain 2: Iteration: 2700 / 5000 [ 54%]  (Sampling)
Chain 3: Iteration: 2700 / 5000 [ 54%]  (Sampling)
Chain 1: Iteration: 2800 / 5000 [ 56%]  (Sampling)
Chain 4: Iteration: 2900 / 5000 [ 58%]  (Sampling)
Chain 2: Iteration: 2800 / 5000 [ 56%]  (Sampling)
Chain 3: Iteration: 2800 / 5000 [ 56%]  (Sampling)
Chain 1: Iteration: 2900 / 5000 [ 58%]  (Sampling)
Chain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)
Chain 2: Iteration: 2900 / 5000 [ 58%]  (Sampling)
Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
Chain 3: Iteration: 2900 / 5000 [ 58%]  (Sampling)
Chain 4: Iteration: 3100 / 5000 [ 62%]  (Sampling)
Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
Chain 1: Iteration: 3100 / 5000 [ 62%]  (Sampling)
Chain 4: Iteration: 3200 / 5000 [ 64%]  (Sampling)
Chain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)
Chain 2: Iteration: 3100 / 5000 [ 62%]  (Sampling)
Chain 1: Iteration: 3200 / 5000 [ 64%]  (Sampling)
Chain 4: Iteration: 3300 / 5000 [ 66%]  (Sampling)
Chain 2: Iteration: 3200 / 5000 [ 64%]  (Sampling)
Chain 3: Iteration: 3100 / 5000 [ 62%]  (Sampling)
Chain 1: Iteration: 3300 / 5000 [ 66%]  (Sampling)
Chain 4: Iteration: 3400 / 5000 [ 68%]  (Sampling)
Chain 2: Iteration: 3300 / 5000 [ 66%]  (Sampling)
Chain 3: Iteration: 3200 / 5000 [ 64%]  (Sampling)
Chain 1: Iteration: 3400 / 5000 [ 68%]  (Sampling)
Chain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)
Chain 2: Iteration: 3400 / 5000 [ 68%]  (Sampling)
Chain 3: Iteration: 3300 / 5000 [ 66%]  (Sampling)
Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
Chain 4: Iteration: 3600 / 5000 [ 72%]  (Sampling)
Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
Chain 1: Iteration: 3600 / 5000 [ 72%]  (Sampling)
Chain 3: Iteration: 3400 / 5000 [ 68%]  (Sampling)
Chain 4: Iteration: 3700 / 5000 [ 74%]  (Sampling)
Chain 2: Iteration: 3600 / 5000 [ 72%]  (Sampling)
Chain 1: Iteration: 3700 / 5000 [ 74%]  (Sampling)
Chain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)
Chain 4: Iteration: 3800 / 5000 [ 76%]  (Sampling)
Chain 2: Iteration: 3700 / 5000 [ 74%]  (Sampling)
Chain 1: Iteration: 3800 / 5000 [ 76%]  (Sampling)
Chain 3: Iteration: 3600 / 5000 [ 72%]  (Sampling)
Chain 4: Iteration: 3900 / 5000 [ 78%]  (Sampling)
Chain 2: Iteration: 3800 / 5000 [ 76%]  (Sampling)
Chain 1: Iteration: 3900 / 5000 [ 78%]  (Sampling)
Chain 3: Iteration: 3700 / 5000 [ 74%]  (Sampling)
Chain 2: Iteration: 3900 / 5000 [ 78%]  (Sampling)
Chain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)
Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
Chain 3: Iteration: 3800 / 5000 [ 76%]  (Sampling)
Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
Chain 4: Iteration: 4100 / 5000 [ 82%]  (Sampling)
Chain 1: Iteration: 4100 / 5000 [ 82%]  (Sampling)
Chain 3: Iteration: 3900 / 5000 [ 78%]  (Sampling)
Chain 2: Iteration: 4100 / 5000 [ 82%]  (Sampling)
Chain 4: Iteration: 4200 / 5000 [ 84%]  (Sampling)
Chain 1: Iteration: 4200 / 5000 [ 84%]  (Sampling)
Chain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)
Chain 2: Iteration: 4200 / 5000 [ 84%]  (Sampling)
Chain 4: Iteration: 4300 / 5000 [ 86%]  (Sampling)
Chain 1: Iteration: 4300 / 5000 [ 86%]  (Sampling)
Chain 2: Iteration: 4300 / 5000 [ 86%]  (Sampling)
Chain 3: Iteration: 4100 / 5000 [ 82%]  (Sampling)
Chain 4: Iteration: 4400 / 5000 [ 88%]  (Sampling)
Chain 1: Iteration: 4400 / 5000 [ 88%]  (Sampling)
Chain 2: Iteration: 4400 / 5000 [ 88%]  (Sampling)
Chain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)
Chain 3: Iteration: 4200 / 5000 [ 84%]  (Sampling)
Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
Chain 4: Iteration: 4600 / 5000 [ 92%]  (Sampling)
Chain 1: Iteration: 4600 / 5000 [ 92%]  (Sampling)
Chain 3: Iteration: 4300 / 5000 [ 86%]  (Sampling)
Chain 2: Iteration: 4600 / 5000 [ 92%]  (Sampling)
Chain 4: Iteration: 4700 / 5000 [ 94%]  (Sampling)
Chain 1: Iteration: 4700 / 5000 [ 94%]  (Sampling)
Chain 3: Iteration: 4400 / 5000 [ 88%]  (Sampling)
Chain 2: Iteration: 4700 / 5000 [ 94%]  (Sampling)
Chain 4: Iteration: 4800 / 5000 [ 96%]  (Sampling)
Chain 1: Iteration: 4800 / 5000 [ 96%]  (Sampling)
Chain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)
Chain 2: Iteration: 4800 / 5000 [ 96%]  (Sampling)
Chain 4: Iteration: 4900 / 5000 [ 98%]  (Sampling)
Chain 1: Iteration: 4900 / 5000 [ 98%]  (Sampling)
Chain 3: Iteration: 4600 / 5000 [ 92%]  (Sampling)
Chain 2: Iteration: 4900 / 5000 [ 98%]  (Sampling)
Chain 4: Iteration: 5000 / 5000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 822.949 seconds (Warm-up)
Chain 4:                1730.04 seconds (Sampling)
Chain 4:                2552.99 seconds (Total)
Chain 4: 
Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 850.95 seconds (Warm-up)
Chain 1:                1708.49 seconds (Sampling)
Chain 1:                2559.44 seconds (Total)
Chain 1: 
Chain 3: Iteration: 4700 / 5000 [ 94%]  (Sampling)
Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 876.241 seconds (Warm-up)
Chain 2:                1710.98 seconds (Sampling)
Chain 2:                2587.22 seconds (Total)
Chain 2: 
Chain 3: Iteration: 4800 / 5000 [ 96%]  (Sampling)
Chain 3: Iteration: 4900 / 5000 [ 98%]  (Sampling)
Chain 3: Iteration: 5000 / 5000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 816.237 seconds (Warm-up)
Chain 3:                1874.56 seconds (Sampling)
Chain 3:                2690.79 seconds (Total)
Chain 3: 
Warning messages:
1: The largest R-hat is NA, indicating chains have not mixed.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#r-hat 
2: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
3: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess 
> 
> # parameters of interest
> param.vec <- c('disp_dev', 'a', 'interactions','effect', 'response', 'sigma_alph', 'mu', 'ifm_alpha', 're')
> 
> # Raw output
> #------------
> save(fit, file = paste0('model/output/', comm, '/model_fit.Rdata')) # model fit
> # Save the 'raw' draws from the posterior
> joint.post.draws <- extract.samples(fit)
> save(joint.post.draws, file = paste0('model/output/', comm, '/post_draws.Rdata'))
> 
> # Save mean, 10% and 90% quantiles for each parameter, as well as n_eff and Rhat
> fit_sum <- summary(fit, pars = param.vec, probs = c(0.1, 0.9))$summary
> write.csv(fit_sum, file = paste0('model/output/', comm, '/summary_of_draws.csv'), row.names = T)
> 
> # Save the logarithm of the (unnormalized) posterior density (lp__)
> log_post <- unlist(extract(fit, 'lp__'))
> write.csv(log_post, file = paste0('model/output/', comm, '/log_post.csv'), row.names = F)
> 
> # Validation
> #------------
> # Diagnostics
> stan_diagnostic(fit, paste0('model/validation/', comm, '/'))

Divergences:
0 of 16000 iterations ended with a divergence.

Tree depth:
0 of 16000 iterations saturated the maximum tree depth of 10.

Energy:
E-BFMI indicated no pathological behavior.
0 of 16000 iterations saturated the maximum tree depth of 10.
E-BFMI indicated no pathological behavior.
0 of 16000 iterations ended with a divergence.
[1] "rhat range: "      "0.999768076891595" "1.0039727910732"  
[1] "n_eff range: "    "864.263061910896" "25870.2893346667"
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
null device 
          1 
Warning messages:
1: Removed 457 rows containing non-finite values (stat_bin). 
2: Removed 457 rows containing non-finite values (stat_bin). 
3: Removed 457 rows containing non-finite values (stat_bin). 
4: Removed 2 rows containing missing values (geom_bar). 
> # Traceplots and posterior uncertainty intervals
> stan_model_check(fit, paste0('model/validation/', comm, '/'), params = param.vec)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
    disp_dev.null device            a.null device interactions.null device 
                       1                        1                        1 
      effect.null device     response.null device   sigma_alph.null device 
                       1                        1                        1 
   ifm_alpha.null device           re.null device 
                       1                        1 
> # Posterior predictive check
> stan_post_pred_check(joint.post.draws, paste0('model/validation/', comm, '/'), stan.data)
NULL
null device 
          1 
> 
> # Parameter outputs - draw 1000 samples from the 80% confidence intervals and save 
> #------------------
> # sample raw model params
> sapply(param.vec[param.vec != 'sigma_alph' & param.vec != 'ifm_alpha'], function(p) {
+   
+   p.samples <- apply(joint.post.draws[[p]], 2, function(x){
+     sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+   })
+   write.csv(p.samples, paste0('model/output/', comm, '/', p, '_samples.csv'), 
+             row.names = F)
+   
+ })
$disp_dev
NULL

$a
NULL

$interactions
NULL

$effect
NULL

$response
NULL

$mu
NULL

$re
NULL

> # sigma alph mucks up because it is one value only, ifm_alphas mucks up because it's a 3d array
> sigma_alph <- joint.post.draws$sigma_alph
> sigma_alph <- sample(sigma_alph[sigma_alph > quantile(sigma_alph, 0.1) & sigma_alph < quantile(sigma_alph, 0.9)], size = 1000)
> write.csv(sigma_alph, paste0('model/output/', comm, '/sigma_alph_samples.csv'), 
+           row.names = F)
> 
> 
> # Transformed parameters
> #-----------------------
> # Intrinsic growth rate (lambda)
> growth.rates.samples <- apply(joint.post.draws$a, 2, function(x){
+   sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+ })
> # exponentiate to get lambda
> growth.rates.samples <- exp(growth.rates.samples)
> write.csv(growth.rates.samples, paste0('model/transformed/', comm, '/lambda_samples.csv'), row.names = F)
> 
> 
> # Interactions (alphas)
> # get posterior draws for all interactions (from the IFM)
> alphas <- joint.post.draws$ifm_alpha
> # columns are interactions (in same order as the interactions vector - focals, then neighbours)
> alphas <- as.data.frame(aperm(alphas, perm = c(1, 3, 2)))
> colnames(alphas) <- grep('ifm_alpha', rownames(fit_sum), value = T)
> # take the 80% posterior interval
> alphas <- apply(alphas, 2, function(x) {
+   inter <- x[x > quantile(x, 0.1) & x < quantile(x, 0.9)]
+   if (length(inter > 0)) {sample(inter, size = 1000)} else {rep(0, 1000)}
+   # this is for those unobserved interactions (0)
+ })
> 
> # calculate interactions according to the response effect model
> response <- apply(joint.post.draws$response, 2, function(x) {
+   sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+ })
> effect <- apply(joint.post.draws$effect, 2, function(x) {
+   sample(x[x > quantile(x, 0.1) & x < quantile(x, 0.9)], size = 1000)
+ })
> 
> re_alphas <- lapply(c(1:length(key_speciesID)), function(x) {
+   ri <- response[, x]
+   ri <- sample(ri, length(ri))  # randomly re-order the ri vector
+   return(ri*effect)})
> re_alphas <- do.call(cbind, re_alphas)
> 
> # Verify!
> png(paste0('model/validation/', comm, '/ifm_vs_rem_alphas.png'))
> plot(alphas, re_alphas, xlab = 'IFM alphas', ylab='Response*Effect',
+      xlim = c(min(re_alphas), max(re_alphas)),
+      ylim = c(min(re_alphas), max(re_alphas)))
> abline(0,1)
> dev.off()
null device 
          1 
> 
> # get unobserved estimates only
> unobs <- re_alphas[ , apply(alphas, 2, function(x) {all(x == 0)})]
> png(paste0('model/validation/', comm, '/alpha_est_distr.png'))
> par(mfrow=c(3,1))
> hist(alphas[ , apply(alphas, 2, function(x) {all(x != 0)})], xlab = "", breaks = 30,
+      main = "IFM alphas (0's removed)", xlim = c(min(re_alphas), max(re_alphas)))
> hist(re_alphas[ , apply(alphas, 2, function(x) {all(x != 0)})],  xlab = "", breaks = 30,
+      main = 'REM alphas - Unobserved estimates removed', xlim = c(min(re_alphas), max(re_alphas)))
> hist(unobs,  xlab = "", main = 'REM alphas - Unobserved interactions only', breaks = 30,
+      xlim = c(min(re_alphas), max(re_alphas)))
> dev.off()
null device 
          1 
> 
> # replace unobserved interactions (0 in alphas) with the values predicted by the rem
> alphas[ , apply(alphas, 2, function(x) {all(x == 0)})] <- 
+   re_alphas[ , apply(alphas, 2, function(x) {all(x == 0)})]
> write.csv(alphas, paste0('model/transformed/', comm, '/alpha_samples.csv'), row.names = F)
> 
> # Scale the alphas and save
> scaled_alphas <- scale_interactions(alphas, growth.rates.samples, key_speciesID, key_neighbourID, comm)
Error in if (prod < 0.1) { : missing value where TRUE/FALSE needed
Calls: scale_interactions -> sapply -> lapply -> FUN
Execution halted
Warning message:
system call failed: Cannot allocate memory 
